{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea103bf",
   "metadata": {},
   "source": [
    "## Deriving the Logistic Regression Objective Function using MLE\n",
    "\n",
    "### Logistic Regression Model\n",
    "\n",
    "We use the logistic (sigmoid) function defined as:\n",
    "\n",
    "\\[\n",
    "\\theta(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "For logistic regression, the model is:\n",
    "\n",
    "\\[\n",
    "P(Y = 1 \\mid X = x) = \\theta(w^T x) = \\frac{e^{w^T x}}{1 + e^{w^T x}}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "P(Y = 0 \\mid X = x) = 1 - \\theta(w^T x) = \\theta(-w^T x)\n",
    "\\]\n",
    "\n",
    "These two cases can be combined into a single expression:\n",
    "\n",
    "\\[\n",
    "P(Y = y \\mid X = x) = \\theta(w^T x)^y \\, \\theta(-w^T x)^{1 - y}\n",
    "\\]\n",
    "\n",
    "where \\( y \\in \\{0,1\\} \\).\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "The likelihood of the parameters \\( w \\) is:\n",
    "\n",
    "\\[\n",
    "L(w) = P(Y \\mid X, w) = \\prod_{i=1}^{n} P(y_i \\mid x_i, w)\n",
    "\\]\n",
    "\n",
    "Substituting the logistic model:\n",
    "\n",
    "\\[\n",
    "L(w) = \\prod_{i=1}^{n} \\theta(w^T x_i)^{y_i} \\, \\theta(-w^T x_i)^{1 - y_i}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Log-Likelihood\n",
    "\n",
    "Instead of maximizing the likelihood directly, we maximize the log-likelihood, which\n",
    "converts the product into a sum:\n",
    "\n",
    "\\[\n",
    "\\ell(w) = \\ln L(w)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\ell(w) = \\sum_{i=1}^{n} \\left[\n",
    "y_i \\ln \\theta(w^T x_i) + (1 - y_i)\\ln \\theta(-w^T x_i)\n",
    "\\right]\n",
    "\\]\n",
    "\n",
    "Using the identity \\( \\theta(-z) = 1 - \\theta(z) \\), this becomes:\n",
    "\n",
    "\\[\n",
    "\\ell(w) = \\sum_{i=1}^{n} \\left[\n",
    "y_i \\ln \\theta(w^T x_i) + (1 - y_i)\\ln (1 - \\theta(w^T x_i))\n",
    "\\right]\n",
    "\\]\n",
    "\n",
    "\n",
    "Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood:\n",
    "\n",
    "\\[\n",
    "E(w) = -\\ell(w)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "E(w) = -\\sum_{i=1}^{n} \\left[\n",
    "y_i \\ln \\theta(w^T x_i) + (1 - y_i)\\ln (1 - \\theta(w^T x_i))\n",
    "\\right]\n",
    "\\]\n",
    "\n",
    "This loss function is known as the binary cross-entropy loss, and it is the standard\n",
    "objective function used in logistic regression.\n",
    "\n",
    "---\n",
    "### Final Objective\n",
    "\n",
    "\\[\n",
    "\\boxed{\n",
    "\\min_{w}\n",
    "\\; -\\sum_{i=1}^{n}\n",
    "\\left[\n",
    "y_i \\ln \\theta(w^T x_i) + (1 - y_i)\\ln (1 - \\theta(w^T x_i))\n",
    "\\right]\n",
    "}\n",
    "\\]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
